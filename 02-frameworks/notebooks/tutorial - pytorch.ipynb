{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial for Computer Vision\n",
    "\n",
    "Topics covered include:\n",
    "\n",
    "- PyTorch fundamentals and tensor operations\n",
    "- Neural network building blocks\n",
    "- Computer vision with CIFAR-10 and Fashion-MNIST\n",
    "- LeNet and AlexNet implementations\n",
    "- Complete training, validation, and evaluation pipelines\n",
    "- Comprehensive visualizations\n",
    "\n",
    "**Prerequisites**: Basic Python knowledge\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [PyTorch Basics](#1-pytorch-basics)\n",
    "2. [Tensors and Operations](#2-tensors-and-operations)\n",
    "3. [Automatic Differentiation](#3-automatic-differentiation)\n",
    "4. [Neural Network Fundamentals](#4-neural-network-fundamentals)\n",
    "5. [Dataset and DataLoader](#5-dataset-and-dataloader)\n",
    "6. [Computer Vision Datasets](#6-computer-vision-datasets)\n",
    "7. [LeNet Implementation](#7-lenet-implementation)\n",
    "8. [AlexNet Implementation](#8-alexnet-implementation)\n",
    "9. [Training Pipeline](#9-training-pipeline)\n",
    "10. [Evaluation and Visualization](#10-evaluation-and-visualization)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, FashionMNIST\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Basics\n",
    "\n",
    "PyTorch is a deep learning framework that provides:\n",
    "\n",
    "- **Tensors**: Multi-dimensional arrays similar to NumPy, but with GPU acceleration\n",
    "- **Autograd**: Automatic differentiation for building neural networks\n",
    "- **Neural Network modules**: Pre-built layers and models\n",
    "- **Optimizers**: Algorithms for training neural networks\n",
    "\n",
    "### Workflow Overview\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data] --> B[DataLoader]\n",
    "    B --> C[Model]\n",
    "    C --> D[Loss Function]\n",
    "    D --> E[Optimizer]\n",
    "    E --> F[Backpropagation]\n",
    "    F --> C\n",
    "    C --> G[Predictions]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensors and Operations\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. They are similar to NumPy arrays but can run on GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from list:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3]), dtype: torch.int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From Python lists: each list is a row in the tensor\n",
    "tensor_from_list = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Tensor from list:\")\n",
    "print(tensor_from_list)\n",
    "print(f\"Shape: {tensor_from_list.shape}, dtype: {tensor_from_list.dtype}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeros tensor\n",
    "\n",
    "```\n",
    "torch.zeros(size, dtype, device) - creates a tensor filled with zeros\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- `size`: tuple defining tensor shape\n",
    "- `dtype`: data type (default: torch.float32)\n",
    "- `device`: cpu or cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros tensor:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(3, 4, dtype=torch.float32)\n",
    "print(\"Zeros tensor:\")\n",
    "print(zeros)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ones tensor\n",
    "ones = torch.ones(2, 3)\n",
    "print(\"Ones tensor:\")\n",
    "print(ones)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random normal tensor (mean=0, std=1):\n",
      "tensor([[-1.2639, -1.7205,  0.9888],\n",
      "        [ 0.3760, -0.0761, -0.8899],\n",
      "        [-1.1185,  0.0677, -0.6314]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random tensors\n",
    "# torch.randn(size) - creates tensor with values from standard normal distribution N(0,1)\n",
    "random_normal = torch.randn(3, 3)\n",
    "print(\"Random normal tensor (mean=0, std=1):\")\n",
    "print(random_normal)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random uniform tensor [0, 1):\n",
      "tensor([[0.1675, 0.3780, 0.8059, 0.5116],\n",
      "        [0.9904, 0.8365, 0.6565, 0.1969]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.rand(size) - creates tensor with values from uniform distribution [0, 1)\n",
    "random_uniform = torch.rand(2, 4)\n",
    "print(\"Random uniform tensor [0, 1):\")\n",
    "print(random_uniform)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range tensor [0, 10) with step 2:\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.arange(start, end, step) - creates 1D tensor with evenly spaced values\n",
    "range_tensor = torch.arange(0, 10, 2)\n",
    "print(\"Range tensor [0, 10) with step 2:\")\n",
    "print(range_tensor)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linspace tensor [0, 1] with 5 steps:\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# torch.linspace(start, end, steps) - creates 1D tensor with linearly spaced values\n",
    "# Parameters:\n",
    "#   start: starting value\n",
    "#   end: ending value (included)\n",
    "#   steps: number of points\n",
    "linspace_tensor = torch.linspace(0, 1, 5)\n",
    "print(\"Linspace tensor [0, 1] with 5 steps:\")\n",
    "print(linspace_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tensor Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "Tensor b:\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise operations\n",
    "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(\"Tensor a:\")\n",
    "print(a)\n",
    "print(\"\\nTensor b:\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a + b (element-wise addition):\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "print(\"\\na + b (element-wise addition):\")\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a * b (element-wise multiplication):\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n"
     ]
    }
   ],
   "source": [
    "# Multiplication (element-wise)\n",
    "print(\"\\na * b (element-wise multiplication):\")\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a @ b.T (matrix multiplication):\n",
      "tensor([[17., 23.],\n",
      "        [39., 53.]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "# torch.matmul(a, b) or a @ b - performs matrix multiplication\n",
    "# For 2D tensors: (m, n) @ (n, p) -> (m, p)\n",
    "print(\"\\na @ b.T (matrix multiplication):\")\n",
    "print(a @ b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original tensor:\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "Reshaped to 3x4:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Reshaped to 2x6:\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "# tensor.view(new_shape) - returns view with new shape (shares memory)\n",
    "# tensor.reshape(new_shape) - returns tensor with new shape (may copy)\n",
    "c = torch.arange(12)\n",
    "print(\"\\nOriginal tensor:\")\n",
    "print(c)\n",
    "print(\"\\nReshaped to 3x4:\")\n",
    "print(c.view(3, 4))\n",
    "print(\"\\nReshaped to 2x6:\")\n",
    "print(c.reshape(2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transpose:\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "# tensor.T - returns transposed tensor\n",
    "# tensor.transpose(dim0, dim1) - swaps dimensions\n",
    "print(\"\\nTranspose:\")\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original matrix:\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "\n",
      "First two rows:\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "\n",
      "Last column:\n",
      "tensor([ 4,  9, 14, 19])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "matrix = torch.arange(20).reshape(4, 5)\n",
    "print(\"\\nOriginal matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nFirst two rows:\")\n",
    "print(matrix[:2, :])\n",
    "print(\"\\nLast column:\")\n",
    "print(matrix[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tensor Reductions and Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(3, 4)\n",
    "print(\"Data tensor:\")\n",
    "print(data)\n",
    "\n",
    "# tensor.sum(dim) - sums elements along dimension\n",
    "# Parameters:\n",
    "#   dim: dimension to reduce (None = all dimensions)\n",
    "#   keepdim: whether to keep reduced dimension as size 1\n",
    "print(\"\\nSum of all elements:\")\n",
    "print(data.sum())\n",
    "print(\"\\nSum along rows (dim=0):\")\n",
    "print(data.sum(dim=0))\n",
    "print(\"\\nSum along columns (dim=1):\")\n",
    "print(data.sum(dim=1))\n",
    "\n",
    "# tensor.mean(dim) - computes mean along dimension\n",
    "print(\"\\nMean:\")\n",
    "print(data.mean())\n",
    "\n",
    "# tensor.std(dim) - computes standard deviation\n",
    "print(\"\\nStandard deviation:\")\n",
    "print(data.std())\n",
    "\n",
    "# tensor.max(dim) - returns maximum values and indices\n",
    "# Returns: (values, indices)\n",
    "print(\"\\nMaximum value:\")\n",
    "print(data.max())\n",
    "print(\"\\nMaximum values and indices along dim=1:\")\n",
    "max_vals, max_indices = data.max(dim=1)\n",
    "print(f\"Values: {max_vals}\")\n",
    "print(f\"Indices: {max_indices}\")\n",
    "\n",
    "# tensor.argmax(dim) - returns index of maximum value\n",
    "print(\"\\nArgmax along dim=1:\")\n",
    "print(data.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Differentiation\n",
    "\n",
    "PyTorch's autograd package provides automatic differentiation for all operations on Tensors. This is essential for training neural networks using backpropagation.\n",
    "\n",
    "### Autograd Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Input Tensor<br/>requires_grad=True] --> B[Forward Pass<br/>Computations]\n",
    "    B --> C[Loss Calculation]\n",
    "    C --> D[loss.backward<br/>Compute Gradients]\n",
    "    D --> E[Gradients Stored<br/>in tensor.grad]\n",
    "    E --> F[Optimizer Step<br/>Update Parameters]\n",
    "    F --> G[optimizer.zero_grad<br/>Clear Gradients]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Basic Autograd Example\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "# requires_grad=True tells PyTorch to track all operations for gradient computation\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\\n\")\n",
    "\n",
    "# Perform operations (PyTorch builds computation graph automatically)\n",
    "y = x ** 2  # y = x^2\n",
    "z = y.sum()  # z = sum(y) = x1^2 + x2^2\n",
    "\n",
    "print(f\"y = x^2: {y}\")\n",
    "print(f\"z = sum(y): {z}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\\n\")  # Shows the operation that created z\n",
    "\n",
    "# Compute gradients\n",
    "# z.backward() computes dz/dx for all tensors with requires_grad=True\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "# For z = x1^2 + x2^2, dz/dx1 = 2*x1, dz/dx2 = 2*x2\n",
    "print(f\"Gradient dz/dx: {x.grad}\")\n",
    "print(f\"Expected gradient [2*x1, 2*x2]: {2 * x.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 More Complex Example\n",
    "\n",
    "# Create input tensor\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "# Complex operation\n",
    "y = x * 2  # y = 2x\n",
    "z = y ** 2  # z = (2x)^2 = 4x^2\n",
    "out = z.mean()  # out = mean(4x^2)\n",
    "\n",
    "print(f\"x:\\n{x}\\n\")\n",
    "print(f\"out: {out}\\n\")\n",
    "\n",
    "# Backpropagate\n",
    "out.backward()\n",
    "\n",
    "# For out = mean(4x^2) = (1/4) * sum(4x^2)\n",
    "# dout/dx = (1/4) * 8x = 2x\n",
    "print(f\"Gradient dout/dx:\\n{x.grad}\")\n",
    "print(f\"\\nExpected gradient (2x):\\n{2 * x.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3 Gradient Management\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "# torch.no_grad() - context manager to disable gradient tracking\n",
    "# Use this during inference to save memory and computation\n",
    "print(\"With gradient tracking:\")\n",
    "y = x + 2\n",
    "print(f\"y.requires_grad: {y.requires_grad}\\n\")\n",
    "\n",
    "print(\"Without gradient tracking:\")\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(f\"y.requires_grad: {y.requires_grad}\\n\")\n",
    "\n",
    "# tensor.detach() - creates a new tensor that doesn't require gradients\n",
    "y = x + 2\n",
    "y_detached = y.detach()\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"y_detached.requires_grad: {y_detached.requires_grad}\\n\")\n",
    "\n",
    "# Zero gradients (important for training loops)\n",
    "# Gradients accumulate by default, so we need to zero them\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "for i in range(3):\n",
    "    y = (x ** 2).sum()\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}, gradient: {x.grad}\")\n",
    "    if i < 2:  # Don't zero on last iteration to show accumulation\n",
    "        x.grad.zero_()  # In-place operation to zero gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Fundamentals\n",
    "\n",
    "PyTorch provides the `torch.nn` module for building neural networks. Key components:\n",
    "\n",
    "- **nn.Module**: Base class for all neural network modules\n",
    "- **nn.Linear**: Fully connected layer\n",
    "- **nn.Conv2d**: 2D convolutional layer\n",
    "- **nn.ReLU**: Activation functions\n",
    "- **nn.CrossEntropyLoss**: Loss functions\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Input<br/>x] --> B[Linear Layer<br/>Wx + b]\n",
    "    B --> C[Activation<br/>ReLU/Sigmoid]\n",
    "    C --> D[Hidden Layer]\n",
    "    D --> E[Output Layer]\n",
    "    E --> F[Loss Function]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1 Simple Linear Layer\n",
    "\n",
    "# nn.Linear(in_features, out_features, bias=True)\n",
    "# Parameters:\n",
    "#   in_features: size of input features\n",
    "#   out_features: size of output features\n",
    "#   bias: whether to include bias term (default: True)\n",
    "# Computes: y = xW^T + b\n",
    "\n",
    "linear = nn.Linear(in_features=5, out_features=3)\n",
    "\n",
    "print(\"Linear layer structure:\")\n",
    "print(linear)\n",
    "print(f\"\\nWeight shape: {linear.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")  # (out_features,)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(2, 5)  # Batch of 2 samples, 5 features each\n",
    "output = linear(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # (2, 3)\n",
    "print(f\"\\nOutput:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2 Activation Functions\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# ReLU: max(0, x)\n",
    "relu = nn.ReLU()\n",
    "relu_output = relu(x)\n",
    "\n",
    "# Sigmoid: 1 / (1 + exp(-x))\n",
    "sigmoid = nn.Sigmoid()\n",
    "sigmoid_output = sigmoid(x)\n",
    "\n",
    "# Tanh: (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "tanh = nn.Tanh()\n",
    "tanh_output = tanh(x)\n",
    "\n",
    "# LeakyReLU: max(0.01*x, x)\n",
    "# nn.LeakyReLU(negative_slope=0.01)\n",
    "# Parameters:\n",
    "#   negative_slope: slope for negative values (default: 0.01)\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "leaky_relu_output = leaky_relu(x)\n",
    "\n",
    "# Visualize activation functions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(x.numpy(), relu_output.numpy(), label='ReLU', color='blue')\n",
    "axes[0, 0].set_title('ReLU Activation')\n",
    "axes[0, 0].grid(True)\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(x.numpy(), sigmoid_output.numpy(), label='Sigmoid', color='green')\n",
    "axes[0, 1].set_title('Sigmoid Activation')\n",
    "axes[0, 1].grid(True)\n",
    "axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(x.numpy(), tanh_output.numpy(), label='Tanh', color='red')\n",
    "axes[1, 0].set_title('Tanh Activation')\n",
    "axes[1, 0].grid(True)\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(x.numpy(), leaky_relu_output.numpy(), label='Leaky ReLU', color='purple')\n",
    "axes[1, 1].set_title('Leaky ReLU Activation')\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Activation Functions Summary:\")\n",
    "print(\"- ReLU: Simple, fast, but can cause 'dying ReLU' problem\")\n",
    "print(\"- Sigmoid: Outputs in (0,1), but suffers from vanishing gradients\")\n",
    "print(\"- Tanh: Outputs in (-1,1), zero-centered, but also vanishing gradients\")\n",
    "print(\"- Leaky ReLU: Prevents dying ReLU by allowing small negative values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.3 Building a Simple Neural Network\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network with two hidden layers.\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> Linear(input_size, 128) -> ReLU -> Linear(128, 64) -> ReLU -> Linear(64, num_classes)\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Number of input features\n",
    "        num_classes (int): Number of output classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()  # Initialize parent class\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 64)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(64, num_classes)  # Output layer\n",
    "        \n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)      # Linear transformation\n",
    "        x = self.relu(x)     # Non-linear activation\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)      # Output layer (no activation, we'll use it with CrossEntropyLoss)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleNN(input_size=784, num_classes=10)  # For MNIST-like data (28x28 = 784)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(32, 784)  # Batch of 32 samples\n",
    "output = model(dummy_input)\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4 Loss Functions and Optimizers\n",
    "\n",
    "# Classification loss functions\n",
    "print(\"=== Loss Functions ===\")\n",
    "\n",
    "# nn.CrossEntropyLoss(weight=None, reduction='mean')\n",
    "# Combines nn.LogSoftmax() and nn.NLLLoss()\n",
    "# Parameters:\n",
    "#   weight: manual rescaling weight for each class\n",
    "#   reduction: 'mean' (default), 'sum', or 'none'\n",
    "# Input: (batch_size, num_classes) - raw scores (logits)\n",
    "# Target: (batch_size,) - class indices\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example usage\n",
    "logits = torch.randn(4, 3)  # 4 samples, 3 classes\n",
    "targets = torch.tensor([0, 1, 2, 1])  # True class indices\n",
    "loss_ce = criterion_ce(logits, targets)\n",
    "print(f\"\\nCrossEntropyLoss: {loss_ce.item():.4f}\")\n",
    "\n",
    "# nn.MSELoss() - Mean Squared Error for regression\n",
    "# Computes: mean((prediction - target)^2)\n",
    "criterion_mse = nn.MSELoss()\n",
    "predictions = torch.randn(4, 1)\n",
    "targets_reg = torch.randn(4, 1)\n",
    "loss_mse = criterion_mse(predictions, targets_reg)\n",
    "print(f\"MSELoss: {loss_mse.item():.4f}\")\n",
    "\n",
    "# nn.BCEWithLogitsLoss() - Binary cross-entropy with logits\n",
    "# More numerically stable than separate sigmoid + BCE\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "logits_binary = torch.randn(4, 1)\n",
    "targets_binary = torch.randint(0, 2, (4, 1)).float()\n",
    "loss_bce = criterion_bce(logits_binary, targets_binary)\n",
    "print(f\"BCEWithLogitsLoss: {loss_bce.item():.4f}\")\n",
    "\n",
    "print(\"\\n=== Optimizers ===\")\n",
    "\n",
    "# Create a simple model\n",
    "model = SimpleNN(784, 10)\n",
    "\n",
    "# optim.SGD(params, lr, momentum=0, weight_decay=0)\n",
    "# Stochastic Gradient Descent\n",
    "# Parameters:\n",
    "#   params: model parameters to optimize\n",
    "#   lr: learning rate\n",
    "#   momentum: momentum factor (default: 0)\n",
    "#   weight_decay: L2 regularization (default: 0)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "print(f\"SGD optimizer: {optimizer_sgd}\")\n",
    "\n",
    "# optim.Adam(params, lr=0.001, betas=(0.9, 0.999), weight_decay=0)\n",
    "# Adaptive Moment Estimation\n",
    "# Parameters:\n",
    "#   params: model parameters to optimize\n",
    "#   lr: learning rate (default: 0.001)\n",
    "#   betas: coefficients for computing running averages\n",
    "#   weight_decay: L2 regularization (default: 0)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(f\"\\nAdam optimizer: {optimizer_adam}\")\n",
    "\n",
    "# optim.RMSprop(params, lr=0.01, alpha=0.99, weight_decay=0)\n",
    "# Root Mean Square Propagation\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n",
    "print(f\"\\nRMSprop optimizer: {optimizer_rmsprop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader\n",
    "\n",
    "PyTorch provides utilities for loading and preprocessing data efficiently.\n",
    "\n",
    "### Data Loading Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw Data] --> B[Dataset Class<br/>__getitem__, __len__]\n",
    "    B --> C[Transforms<br/>Preprocessing]\n",
    "    C --> D[DataLoader<br/>Batching & Shuffling]\n",
    "    D --> E[Model Training]\n",
    "\n",
    "    style D fill:#e1f5ff\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1 Custom Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for demonstration.\n",
    "    \n",
    "    All custom datasets must implement:\n",
    "    - __init__: Initialize dataset (load data, setup transforms, etc.)\n",
    "    - __len__: Return the size of the dataset\n",
    "    - __getitem__: Return a single sample at given index\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate\n",
    "        num_features (int): Number of features per sample\n",
    "        num_classes (int): Number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=1000, num_features=20, num_classes=3):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Generate random data\n",
    "        self.data = torch.randn(num_samples, num_features)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (data, label)\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = CustomDataset(num_samples=1000, num_features=20, num_classes=3)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Get a single sample\n",
    "sample, label = dataset[0]\n",
    "print(f\"Sample shape: {sample.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 DataLoader\n",
    "\n",
    "# DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "# Parameters:\n",
    "#   dataset: dataset to load from\n",
    "#   batch_size: number of samples per batch\n",
    "#   shuffle: whether to shuffle data at every epoch\n",
    "#   num_workers: number of subprocesses for data loading (0 = main process)\n",
    "#   drop_last: drop the last incomplete batch if dataset size not divisible by batch_size\n",
    "#   pin_memory: if True, copies tensors into CUDA pinned memory (faster transfer to GPU)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for Windows, 2-4 for Linux/Mac\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "    print(f\"  Data shape: {data.shape}\")      # (batch_size, num_features)\n",
    "    print(f\"  Labels shape: {labels.shape}\")  # (batch_size,)\n",
    "    \n",
    "    if batch_idx == 2:  # Show only first 3 batches\n",
    "        break\n",
    "\n",
    "print(\"\\n...\")\n",
    "print(f\"Total batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Computer Vision Datasets\n",
    "\n",
    "We'll work with two popular computer vision datasets:\n",
    "\n",
    "- **Fashion-MNIST**: 28x28 grayscale images of clothing items (10 classes)\n",
    "- **CIFAR-10**: 32x32 color images of objects (10 classes)\n",
    "\n",
    "### Image Preprocessing Pipeline\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw Image] --> B[Resize/Crop]\n",
    "    B --> C[ToTensor<br/>0-255 â†’ 0-1]\n",
    "    C --> D[Normalize<br/>mean, std]\n",
    "    D --> E[Augmentations<br/>flip, rotate]\n",
    "    E --> F[Model Input]\n",
    "\n",
    "    style C fill:#ffe1e1\n",
    "    style D fill:#ffe1e1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.1 Fashion-MNIST Dataset\n",
    "\n",
    "# transforms.Compose() - chains multiple transforms together\n",
    "# transforms.ToTensor() - converts PIL Image or ndarray to tensor and scales to [0, 1]\n",
    "# transforms.Normalize(mean, std) - normalizes tensor with mean and standard deviation\n",
    "#   output = (input - mean) / std\n",
    "\n",
    "transform_fmnist = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load Fashion-MNIST\n",
    "# FashionMNIST(root, train=True, transform=None, download=True)\n",
    "# Parameters:\n",
    "#   root: directory to save/load data\n",
    "#   train: if True, loads training set; if False, loads test set\n",
    "#   transform: transformations to apply to data\n",
    "#   download: if True, downloads data if not already present\n",
    "train_fmnist = FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform_fmnist,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_fmnist = FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform_fmnist,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(f\"Fashion-MNIST Training set size: {len(train_fmnist)}\")\n",
    "print(f\"Fashion-MNIST Test set size: {len(test_fmnist)}\")\n",
    "\n",
    "# Class labels\n",
    "fmnist_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Fashion-MNIST Sample Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    image, label = train_fmnist[idx]\n",
    "    # Denormalize: image = image * std + mean\n",
    "    image = image * 0.5 + 0.5  # Convert back to [0, 1]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(fmnist_classes[label])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.2 CIFAR-10 Dataset\n",
    "\n",
    "# CIFAR-10 specific transforms with data augmentation\n",
    "# Data augmentation helps prevent overfitting\n",
    "\n",
    "# Training transforms (with augmentation)\n",
    "# transforms.RandomHorizontalFlip(p=0.5) - randomly flip images horizontally\n",
    "# transforms.RandomCrop(size, padding) - randomly crop image after padding\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance of flipping\n",
    "    transforms.RandomCrop(32, padding=4),     # Crop to 32x32 after padding by 4\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],  # CIFAR-10 mean per channel (R, G, B)\n",
    "        std=[0.2470, 0.2435, 0.2616]     # CIFAR-10 std per channel\n",
    "    )\n",
    "])\n",
    "\n",
    "# Test transforms (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_cifar10 = CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform_train,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_cifar10 = CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform_test,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(f\"CIFAR-10 Training set size: {len(train_cifar10)}\")\n",
    "print(f\"CIFAR-10 Test set size: {len(test_cifar10)}\")\n",
    "\n",
    "# Class labels\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('CIFAR-10 Sample Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Use test set for visualization (no augmentation)\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    image, label = test_cifar10[idx]\n",
    "    # Denormalize for visualization\n",
    "    image = image * torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1) + \\\n",
    "            torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    image = torch.clamp(image, 0, 1)  # Ensure values are in [0, 1]\n",
    "    ax.imshow(image.permute(1, 2, 0))  # Change from (C, H, W) to (H, W, C)\n",
    "    ax.set_title(cifar10_classes[label])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.3 Dataset Statistics\n",
    "\n",
    "# Analyze class distribution\n",
    "train_labels_cifar = [label for _, label in train_cifar10]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Class distribution\n",
    "class_counts = np.bincount(train_labels_cifar)\n",
    "ax1.bar(range(10), class_counts, color=sns.color_palette('husl', 10))\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "ax1.set_title('CIFAR-10 Class Distribution')\n",
    "ax1.set_xticks(range(10))\n",
    "ax1.set_xticklabels(cifar10_classes, rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sample image dimensions\n",
    "sample_image, _ = train_cifar10[0]\n",
    "info_text = f\"\"\"CIFAR-10 Dataset Information:\n",
    "\n",
    "Image shape: {sample_image.shape}\n",
    "Channels: {sample_image.shape[0]} (RGB)\n",
    "Height: {sample_image.shape[1]} pixels\n",
    "Width: {sample_image.shape[2]} pixels\n",
    "\n",
    "Training samples: {len(train_cifar10):,}\n",
    "Test samples: {len(test_cifar10):,}\n",
    "Number of classes: 10\n",
    "\n",
    "Samples per class: {class_counts[0]:,}\n",
    "(Balanced dataset)\n",
    "\"\"\"\n",
    "\n",
    "ax2.text(0.1, 0.5, info_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LeNet Implementation\n",
    "\n",
    "LeNet-5 is a pioneering convolutional neural network designed by Yann LeCun in 1998 for handwritten digit recognition.\n",
    "\n",
    "### LeNet Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Input<br/>32x32x1] --> B[Conv1<br/>6@28x28]\n",
    "    B --> C[Pool1<br/>6@14x14]\n",
    "    C --> D[Conv2<br/>16@10x10]\n",
    "    D --> E[Pool2<br/>16@5x5]\n",
    "    E --> F[Flatten<br/>400]\n",
    "    F --> G[FC1<br/>120]\n",
    "    G --> H[FC2<br/>84]\n",
    "    H --> I[FC3<br/>10]\n",
    "\n",
    "    style B fill:#e1f5ff\n",
    "    style D fill:#e1f5ff\n",
    "    style F fill:#ffe1e1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.1 LeNet-5 Model\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 Convolutional Neural Network.\n",
    "    \n",
    "    Original paper: \"Gradient-Based Learning Applied to Document Recognition\" (LeCun et al., 1998)\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1: 1 input channel, 6 output channels, 5x5 kernel\n",
    "    - Pooling: 2x2 average pooling\n",
    "    - Conv2: 6 input channels, 16 output channels, 5x5 kernel\n",
    "    - Pooling: 2x2 average pooling\n",
    "    - FC1: 16*5*5 -> 120\n",
    "    - FC2: 120 -> 84\n",
    "    - FC3: 84 -> num_classes\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of output classes (default: 10)\n",
    "        in_channels (int): Number of input channels (default: 1 for grayscale)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, in_channels=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        # Parameters:\n",
    "        #   in_channels: number of channels in input\n",
    "        #   out_channels: number of channels produced by convolution\n",
    "        #   kernel_size: size of convolving kernel\n",
    "        #   stride: stride of convolution (default: 1)\n",
    "        #   padding: zero-padding added to both sides (default: 0)\n",
    "        # Output size: (input_size - kernel_size + 2*padding) / stride + 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 6, kernel_size=5, stride=1, padding=0)\n",
    "        # Input: (batch, in_channels, 32, 32) -> Output: (batch, 6, 28, 28)\n",
    "        \n",
    "        # nn.AvgPool2d(kernel_size, stride=None)\n",
    "        # Average pooling operation\n",
    "        # If stride is None, it's set to kernel_size\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # Input: (batch, 6, 28, 28) -> Output: (batch, 6, 14, 14)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        # Input: (batch, 6, 14, 14) -> Output: (batch, 16, 10, 10)\n",
    "        # After pool: (batch, 16, 5, 5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Convolutional layers with activation and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (batch, 6, 14, 14)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (batch, 16, 5, 5)\n",
    "        \n",
    "        # Flatten: reshape to (batch, 16*5*5)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation (use with CrossEntropyLoss)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create LeNet model for Fashion-MNIST (grayscale)\n",
    "lenet_fmnist = LeNet5(num_classes=10, in_channels=1)\n",
    "print(\"LeNet-5 for Fashion-MNIST:\")\n",
    "print(lenet_fmnist)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lenet_fmnist.parameters()):,}\")\n",
    "\n",
    "# Create LeNet model for CIFAR-10 (color)\n",
    "lenet_cifar = LeNet5(num_classes=10, in_channels=3)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LeNet-5 for CIFAR-10:\")\n",
    "print(lenet_cifar)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lenet_cifar.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input_gray = torch.randn(4, 1, 32, 32)  # Batch of 4 grayscale images\n",
    "dummy_input_color = torch.randn(4, 3, 32, 32)  # Batch of 4 color images\n",
    "\n",
    "output_gray = lenet_fmnist(dummy_input_gray)\n",
    "output_color = lenet_cifar(dummy_input_color)\n",
    "\n",
    "print(f\"\\nGrayscale input shape: {dummy_input_gray.shape}\")\n",
    "print(f\"Grayscale output shape: {output_gray.shape}\")\n",
    "print(f\"\\nColor input shape: {dummy_input_color.shape}\")\n",
    "print(f\"Color output shape: {output_color.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.2 Visualize LeNet Feature Maps\n",
    "\n",
    "def visualize_feature_maps(model, image, layer_name='conv1'):\n",
    "    \"\"\"\n",
    "    Visualize feature maps from a convolutional layer.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        image: Input image tensor (1, C, H, W)\n",
    "        layer_name: Name of the layer to visualize\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass through first conv layer\n",
    "    if layer_name == 'conv1':\n",
    "        features = model.pool(F.relu(model.conv1(image)))\n",
    "        title = 'Conv1 Feature Maps (after pooling)'\n",
    "    else:\n",
    "        x = model.pool(F.relu(model.conv1(image)))\n",
    "        features = model.pool(F.relu(model.conv2(x)))\n",
    "        title = 'Conv2 Feature Maps (after pooling)'\n",
    "    \n",
    "    # Convert to numpy\n",
    "    features = features.detach().cpu().numpy()[0]  # Remove batch dimension\n",
    "    \n",
    "    # Plot feature maps\n",
    "    num_features = features.shape[0]\n",
    "    cols = 8 if num_features == 16 else 6\n",
    "    rows = (num_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 1.5))\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx in range(rows * cols):\n",
    "        ax = axes.flat[idx] if rows > 1 else axes[idx]\n",
    "        if idx < num_features:\n",
    "            ax.imshow(features[idx], cmap='viridis')\n",
    "            ax.set_title(f'Filter {idx+1}', fontsize=8)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample image from Fashion-MNIST\n",
    "sample_image, label = train_fmnist[0]\n",
    "sample_image = sample_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Visualize original image\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample_image.squeeze().numpy() * 0.5 + 0.5, cmap='gray')\n",
    "plt.title(f'Original Image: {fmnist_classes[label]}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "visualize_feature_maps(lenet_fmnist, sample_image, 'conv1')\n",
    "visualize_feature_maps(lenet_fmnist, sample_image, 'conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AlexNet Implementation\n",
    "\n",
    "AlexNet is a deeper convolutional neural network that won the ImageNet competition in 2012, marking a breakthrough in deep learning.\n",
    "\n",
    "### AlexNet Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Input<br/>3x32x32] --> B[Conv1<br/>64@32x32]\n",
    "    B --> C[Pool1<br/>64@16x16]\n",
    "    C --> D[Conv2<br/>192@16x16]\n",
    "    D --> E[Pool2<br/>192@8x8]\n",
    "    E --> F[Conv3<br/>384@8x8]\n",
    "    F --> G[Conv4<br/>256@8x8]\n",
    "    G --> H[Conv5<br/>256@8x8]\n",
    "    H --> I[Pool3<br/>256@4x4]\n",
    "    I --> J[Flatten]\n",
    "    J --> K[FC1<br/>4096]\n",
    "    K --> L[Dropout]\n",
    "    L --> M[FC2<br/>4096]\n",
    "    M --> N[Dropout]\n",
    "    N --> O[FC3<br/>10]\n",
    "\n",
    "    style B fill:#e1f5ff\n",
    "    style D fill:#e1f5ff\n",
    "    style F fill:#e1f5ff\n",
    "    style L fill:#ffe1e1\n",
    "    style N fill:#ffe1e1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.1 AlexNet Model (Adapted for CIFAR-10)\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet Convolutional Neural Network (adapted for CIFAR-10 32x32 images).\n",
    "    \n",
    "    Original paper: \"ImageNet Classification with Deep Convolutional Neural Networks\" \n",
    "    (Krizhevsky et al., 2012)\n",
    "    \n",
    "    Modifications for CIFAR-10:\n",
    "    - Adjusted layer sizes for 32x32 input instead of 224x224\n",
    "    - Reduced fully connected layer sizes\n",
    "    - Added batch normalization for better training\n",
    "    \n",
    "    Architecture:\n",
    "    - 5 Convolutional layers\n",
    "    - 3 Fully connected layers\n",
    "    - ReLU activations\n",
    "    - Dropout for regularization\n",
    "    - Max pooling\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of output classes (default: 10)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 3 -> 64 channels\n",
    "            # nn.Conv2d maintains spatial dimensions with padding=2\n",
    "            # Input: (batch, 3, 32, 32) -> Output: (batch, 64, 32, 32)\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size, stride) - takes maximum value in window\n",
    "            # Input: (batch, 64, 32, 32) -> Output: (batch, 64, 16, 16)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv2: 64 -> 192 channels\n",
    "            # Input: (batch, 64, 16, 16) -> Output: (batch, 192, 16, 16)\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Input: (batch, 192, 16, 16) -> Output: (batch, 192, 8, 8)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv3: 192 -> 384 channels\n",
    "            # Input: (batch, 192, 8, 8) -> Output: (batch, 384, 8, 8)\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv4: 384 -> 256 channels\n",
    "            # Input: (batch, 384, 8, 8) -> Output: (batch, 256, 8, 8)\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv5: 256 -> 256 channels\n",
    "            # Input: (batch, 256, 8, 8) -> Output: (batch, 256, 8, 8)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Input: (batch, 256, 8, 8) -> Output: (batch, 256, 4, 4)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # nn.AdaptiveAvgPool2d(output_size) - adaptive pooling to fixed output size\n",
    "        # Useful when input size varies\n",
    "        # Input: (batch, 256, 4, 4) -> Output: (batch, 256, 2, 2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(p=0.5) - randomly zeroes elements with probability p\n",
    "            # Parameters:\n",
    "            #   p: probability of an element to be zeroed (default: 0.5)\n",
    "            # Helps prevent overfitting during training\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256 * 2 * 2, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 3, 32, 32)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.features(x)      # Convolutional layers\n",
    "        x = self.avgpool(x)       # Adaptive pooling\n",
    "        x = torch.flatten(x, 1)   # Flatten all dimensions except batch\n",
    "        x = self.classifier(x)    # Fully connected layers\n",
    "        return x\n",
    "\n",
    "# Create AlexNet model\n",
    "alexnet = AlexNet(num_classes=10)\n",
    "print(\"AlexNet for CIFAR-10:\")\n",
    "print(alexnet)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in alexnet.parameters()):,}\")\n",
    "\n",
    "# Count parameters by layer type\n",
    "conv_params = sum(p.numel() for name, p in alexnet.named_parameters() if 'features' in name)\n",
    "fc_params = sum(p.numel() for name, p in alexnet.named_parameters() if 'classifier' in name)\n",
    "print(f\"\\nConvolutional layers parameters: {conv_params:,}\")\n",
    "print(f\"Fully connected layers parameters: {fc_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(4, 3, 32, 32)\n",
    "output = alexnet(dummy_input)\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Pipeline\n",
    "\n",
    "Complete training pipeline with training, validation, and evaluation on CIFAR-10 dataset.\n",
    "\n",
    "### Training Pipeline Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Start Epoch] --> B[Set model.train]\n",
    "    B --> C[Iterate Training Batches]\n",
    "    C --> D[Forward Pass]\n",
    "    D --> E[Compute Loss]\n",
    "    E --> F[Backward Pass]\n",
    "    F --> G[Optimizer Step]\n",
    "    G --> H{More Batches?}\n",
    "    H -->|Yes| C\n",
    "    H -->|No| I[Validation Phase]\n",
    "    I --> J[Set model.eval]\n",
    "    J --> K[torch.no_grad]\n",
    "    K --> L[Iterate Val Batches]\n",
    "    L --> M[Forward Pass]\n",
    "    M --> N[Compute Metrics]\n",
    "    N --> O{More Batches?}\n",
    "    O -->|Yes| L\n",
    "    O -->|No| P[Log Results]\n",
    "    P --> Q{More Epochs?}\n",
    "    Q -->|Yes| A\n",
    "    Q -->|No| R[Final Evaluation]\n",
    "\n",
    "    style B fill:#e1f5ff\n",
    "    style J fill:#ffe1e1\n",
    "    style F fill:#fff4e1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.1 Prepare CIFAR-10 Data with Train/Validation Split\n",
    "\n",
    "# Create data directory\n",
    "data_dir = Path('./data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Reload CIFAR-10 with proper transforms\n",
    "transform_train_cifar = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Additional augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
    "])\n",
    "\n",
    "transform_test_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "full_train_dataset = CIFAR10(root='./data', train=True, transform=transform_train_cifar, download=True)\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=transform_test_cifar, download=True)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "# random_split(dataset, lengths) - randomly splits dataset into non-overlapping new datasets\n",
    "# Parameters:\n",
    "#   dataset: dataset to be split\n",
    "#   lengths: lengths of splits to be produced (list or tuple)\n",
    "train_size = int(0.9 * len(full_train_dataset))  # 90% for training\n",
    "val_size = len(full_train_dataset) - train_size  # 10% for validation\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset):,}\")\n",
    "print(f\"Validation set size: {len(val_dataset):,}\")\n",
    "print(f\"Test set size: {len(test_dataset):,}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,      # Shuffle training data\n",
    "    num_workers=0,     # Number of parallel workers\n",
    "    pin_memory=True    # Faster transfer to GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,     # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.2 Training and Validation Functions\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        device: Device to run on (cpu or cuda)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode (enables dropout, batch norm updates)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # tqdm provides a progress bar\n",
    "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        # IMPORTANT: Must be done before backward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()  # Compute gradients\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)  # Get class with highest score\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / (batch_idx + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: Device to run on (cpu or cuda)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm uses running stats)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # torch.no_grad() disables gradient computation (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        \n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass only\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / len(val_loader),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "    \n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Complete training loop with validation.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs (int): Number of epochs to train\n",
    "        device: Device to run on (cpu or cuda)\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Training history containing losses and accuracies\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)  # Some schedulers use validation loss\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Learning rate: {current_lr:.6f}')\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
    "            # In practice, you would save the model here:\n",
    "            # torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f'\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    return history\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.3 Train LeNet-5 on CIFAR-10\n",
    "\n",
    "# Initialize model\n",
    "lenet_model = LeNet5(num_classes=10, in_channels=3).to(device)\n",
    "print(f\"Training LeNet-5 on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in lenet_model.parameters()):,}\\n\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_lenet = optim.Adam(lenet_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "# optim.lr_scheduler.ReduceLROnPlateau - reduces learning rate when metric plateaus\n",
    "# Parameters:\n",
    "#   optimizer: wrapped optimizer\n",
    "#   mode: 'min' for loss, 'max' for accuracy\n",
    "#   factor: factor by which to reduce learning rate (new_lr = lr * factor)\n",
    "#   patience: number of epochs with no improvement after which lr is reduced\n",
    "#   verbose: if True, prints message when lr is reduced\n",
    "scheduler_lenet = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_lenet, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 15\n",
    "print(f\"Training for {num_epochs} epochs...\\n\")\n",
    "\n",
    "history_lenet = train_model(\n",
    "    model=lenet_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_lenet,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    scheduler=scheduler_lenet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.4 Train AlexNet on CIFAR-10\n",
    "\n",
    "# Initialize model\n",
    "alexnet_model = AlexNet(num_classes=10).to(device)\n",
    "print(f\"Training AlexNet on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in alexnet_model.parameters()):,}\\n\")\n",
    "\n",
    "# Define optimizer with different learning rate\n",
    "optimizer_alexnet = optim.Adam(alexnet_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_alexnet = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_alexnet, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training for {num_epochs} epochs...\\n\")\n",
    "\n",
    "history_alexnet = train_model(\n",
    "    model=alexnet_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_alexnet,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    scheduler=scheduler_alexnet\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation and Visualization\n",
    "\n",
    "Comprehensive evaluation of trained models with detailed visualizations using matplotlib and seaborn.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Model Predictions] --> B[Accuracy]\n",
    "    A --> C[Precision]\n",
    "    A --> D[Recall]\n",
    "    A --> E[F1-Score]\n",
    "    A --> F[Confusion Matrix]\n",
    "\n",
    "    B --> G[Overall Performance]\n",
    "    C --> G\n",
    "    D --> G\n",
    "    E --> G\n",
    "    F --> H[Per-Class Analysis]\n",
    "\n",
    "    style G fill:#e1ffe1\n",
    "    style H fill:#ffe1e1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.1 Plot Training History\n",
    "\n",
    "def plot_training_history(history, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Training history with keys: train_loss, train_acc, val_loss, val_acc\n",
    "        model_name (str): Name of the model for plot title\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-o', label='Training Loss', linewidth=2, markersize=6)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-s', label='Validation Loss', linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title(f'{model_name} - Loss Curves', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy', linewidth=2, markersize=6)\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-s', label='Validation Accuracy', linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title(f'{model_name} - Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{model_name} Training Summary:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "    print(f\"\\nBest Validation Accuracy: {max(history['val_acc']):.2f}% (Epoch {np.argmax(history['val_acc']) + 1})\")\n",
    "    print(f\"Best Validation Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss']) + 1})\")\n",
    "\n",
    "# Plot LeNet training history\n",
    "plot_training_history(history_lenet, 'LeNet-5')\n",
    "\n",
    "# Plot AlexNet training history\n",
    "plot_training_history(history_alexnet, 'AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.2 Compare Models\n",
    "\n",
    "def compare_models(histories, model_names):\n",
    "    \"\"\"\n",
    "    Compare training histories of multiple models.\n",
    "    \n",
    "    Args:\n",
    "        histories (list): List of history dictionaries\n",
    "        model_names (list): List of model names\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(histories[0]['train_loss']) + 1)\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "    \n",
    "    # Training Loss\n",
    "    for idx, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        axes[0, 0].plot(epochs, history['train_loss'], \n",
    "                       label=name, linewidth=2, marker='o', color=colors[idx])\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation Loss\n",
    "    for idx, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        axes[0, 1].plot(epochs, history['val_loss'], \n",
    "                       label=name, linewidth=2, marker='s', color=colors[idx])\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=11)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Accuracy\n",
    "    for idx, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        axes[1, 0].plot(epochs, history['train_acc'], \n",
    "                       label=name, linewidth=2, marker='o', color=colors[idx])\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1, 0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation Accuracy\n",
    "    for idx, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        axes[1, 1].plot(epochs, history['val_acc'], \n",
    "                       label=name, linewidth=2, marker='s', color=colors[idx])\n",
    "    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1, 1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Model':<15} {'Final Train Acc':<18} {'Final Val Acc':<18} {'Best Val Acc':<15}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    for history, name in zip(histories, model_names):\n",
    "        print(f\"{name:<15} {history['train_acc'][-1]:>15.2f}% \"\n",
    "              f\"{history['val_acc'][-1]:>15.2f}% {max(history['val_acc']):>12.2f}%\")\n",
    "\n",
    "# Compare LeNet and AlexNet\n",
    "compare_models([history_lenet, history_alexnet], ['LeNet-5', 'AlexNet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.3 Confusion Matrix and Detailed Evaluation\n",
    "\n",
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and compute detailed metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        test_loader (DataLoader): Test data loader\n",
    "        device: Device to run on\n",
    "        class_names (list): List of class names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_predictions, all_targets, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    accuracy = 100. * (all_predictions == all_targets).sum() / len(all_targets)\n",
    "    \n",
    "    return all_predictions, all_targets, accuracy\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(predictions, targets, class_names, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix using seaborn.\n",
    "    \n",
    "    Args:\n",
    "        predictions (array): Predicted labels\n",
    "        targets (array): True labels\n",
    "        class_names (list): List of class names\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(targets, predictions)\n",
    "    \n",
    "    # Normalize by true labels (rows)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=ax1, cbar_kws={'label': 'Count'})\n",
    "    ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax1.set_ylabel('True Label', fontsize=12)\n",
    "    ax1.set_title(f'{model_name} - Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Normalized (percentages)\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=ax2, cbar_kws={'label': 'Percentage'}, vmin=0, vmax=1)\n",
    "    ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax2.set_ylabel('True Label', fontsize=12)\n",
    "    ax2.set_title(f'{model_name} - Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_per_class_metrics(predictions, targets, class_names):\n",
    "    \"\"\"\n",
    "    Compute per-class precision, recall, and F1-score.\n",
    "    \n",
    "    Args:\n",
    "        predictions (array): Predicted labels\n",
    "        targets (array): True labels\n",
    "        class_names (list): List of class names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing per-class metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        targets, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'class_names': class_names,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_per_class_metrics(metrics, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Plot per-class metrics using bar charts.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Dictionary containing per-class metrics\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    x = np.arange(len(metrics['class_names']))\n",
    "    width = 0.6\n",
    "    \n",
    "    # Precision\n",
    "    bars1 = axes[0, 0].bar(x, metrics['precision'], width, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Class', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Precision', fontsize=12)\n",
    "    axes[0, 0].set_title('Precision per Class', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(metrics['class_names'], rotation=45, ha='right')\n",
    "    axes[0, 0].set_ylim([0, 1.05])\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Recall\n",
    "    bars2 = axes[0, 1].bar(x, metrics['recall'], width, color='coral', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Class', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Recall', fontsize=12)\n",
    "    axes[0, 1].set_title('Recall per Class', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(metrics['class_names'], rotation=45, ha='right')\n",
    "    axes[0, 1].set_ylim([0, 1.05])\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # F1-Score\n",
    "    bars3 = axes[1, 0].bar(x, metrics['f1'], width, color='mediumseagreen', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Class', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('F1-Score', fontsize=12)\n",
    "    axes[1, 0].set_title('F1-Score per Class', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(metrics['class_names'], rotation=45, ha='right')\n",
    "    axes[1, 0].set_ylim([0, 1.05])\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # All metrics together\n",
    "    x_multi = np.arange(len(metrics['class_names']))\n",
    "    width_multi = 0.25\n",
    "    \n",
    "    axes[1, 1].bar(x_multi - width_multi, metrics['precision'], width_multi, \n",
    "                   label='Precision', color='steelblue', edgecolor='black')\n",
    "    axes[1, 1].bar(x_multi, metrics['recall'], width_multi, \n",
    "                   label='Recall', color='coral', edgecolor='black')\n",
    "    axes[1, 1].bar(x_multi + width_multi, metrics['f1'], width_multi, \n",
    "                   label='F1-Score', color='mediumseagreen', edgecolor='black')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Class', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1, 1].set_title('All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xticks(x_multi)\n",
    "    axes[1, 1].set_xticklabels(metrics['class_names'], rotation=45, ha='right')\n",
    "    axes[1, 1].set_ylim([0, 1.05])\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Per-Class Performance Metrics', \n",
    "                 fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print metrics table\n",
    "    print(f\"\\n{model_name} - Detailed Per-Class Metrics:\")\n",
    "    print(f\"{'='*90}\")\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(f\"{'-'*90}\")\n",
    "    for i, class_name in enumerate(metrics['class_names']):\n",
    "        print(f\"{class_name:<15} {metrics['precision'][i]:>10.4f}  \"\n",
    "              f\"{metrics['recall'][i]:>10.4f}  {metrics['f1'][i]:>10.4f}  \"\n",
    "              f\"{metrics['support'][i]:>8}\")\n",
    "    print(f\"{'-'*90}\")\n",
    "    print(f\"{'Average':<15} {np.mean(metrics['precision']):>10.4f}  \"\n",
    "          f\"{np.mean(metrics['recall']):>10.4f}  {np.mean(metrics['f1']):>10.4f}  \"\n",
    "          f\"{np.sum(metrics['support']):>8}\")\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.4 Evaluate LeNet-5 on Test Set\n",
    "\n",
    "print(\"Evaluating LeNet-5 on test set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions\n",
    "predictions_lenet, targets_lenet, accuracy_lenet = evaluate_model(\n",
    "    lenet_model, test_loader, device, cifar10_classes\n",
    ")\n",
    "\n",
    "print(f\"\\nLeNet-5 Test Accuracy: {accuracy_lenet:.2f}%\\n\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(predictions_lenet, targets_lenet, cifar10_classes, 'LeNet-5')\n",
    "\n",
    "# Compute and plot per-class metrics\n",
    "metrics_lenet = compute_per_class_metrics(predictions_lenet, targets_lenet, cifar10_classes)\n",
    "plot_per_class_metrics(metrics_lenet, 'LeNet-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.5 Evaluate AlexNet on Test Set\n",
    "\n",
    "print(\"Evaluating AlexNet on test set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions\n",
    "predictions_alexnet, targets_alexnet, accuracy_alexnet = evaluate_model(\n",
    "    alexnet_model, test_loader, device, cifar10_classes\n",
    ")\n",
    "\n",
    "print(f\"\\nAlexNet Test Accuracy: {accuracy_alexnet:.2f}%\\n\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(predictions_alexnet, targets_alexnet, cifar10_classes, 'AlexNet')\n",
    "\n",
    "# Compute and plot per-class metrics\n",
    "metrics_alexnet = compute_per_class_metrics(predictions_alexnet, targets_alexnet, cifar10_classes)\n",
    "plot_per_class_metrics(metrics_alexnet, 'AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.6 Visualize Predictions\n",
    "\n",
    "def visualize_predictions(model, test_dataset, device, class_names, num_images=16, show_correct=True):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on test images.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        test_dataset: Test dataset\n",
    "        device: Device to run on\n",
    "        class_names (list): List of class names\n",
    "        num_images (int): Number of images to display\n",
    "        show_correct (bool): If True, show correct predictions; if False, show incorrect\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Find correct/incorrect predictions\n",
    "    indices = []\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_dataset)):\n",
    "            if len(indices) >= num_images:\n",
    "                break\n",
    "            \n",
    "            image, label = test_dataset[idx]\n",
    "            image_input = image.unsqueeze(0).to(device)\n",
    "            output = model(image_input)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            is_correct = (predicted.item() == label)\n",
    "            if is_correct == show_correct:\n",
    "                indices.append(idx)\n",
    "    \n",
    "    # Plot images\n",
    "    cols = 4\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(14, rows * 3.5))\n",
    "    \n",
    "    title = \"Correct Predictions\" if show_correct else \"Incorrect Predictions\"\n",
    "    fig.suptitle(f'{title} - {model.__class__.__name__}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(indices):\n",
    "            image, label = test_dataset[indices[idx]]\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                image_input = image.unsqueeze(0).to(device)\n",
    "                output = model(image_input)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                confidence, predicted = probabilities.max(1)\n",
    "            \n",
    "            # Denormalize image\n",
    "            image_display = image * torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1) + \\\n",
    "                           torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "            image_display = torch.clamp(image_display, 0, 1)\n",
    "            \n",
    "            # Display image\n",
    "            ax.imshow(image_display.permute(1, 2, 0).cpu().numpy())\n",
    "            \n",
    "            # Set title with prediction info\n",
    "            true_label = class_names[label]\n",
    "            pred_label = class_names[predicted.item()]\n",
    "            conf = confidence.item() * 100\n",
    "            \n",
    "            if show_correct:\n",
    "                title_text = f'True: {true_label}\\\\nPred: {pred_label}\\\\nConf: {conf:.1f}%'\n",
    "                title_color = 'green'\n",
    "            else:\n",
    "                title_text = f'True: {true_label}\\\\nPred: {pred_label}\\\\nConf: {conf:.1f}%'\n",
    "                title_color = 'red'\n",
    "            \n",
    "            ax.set_title(title_text, fontsize=10, color=title_color, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize correct predictions for AlexNet\n",
    "print(\"AlexNet - Correct Predictions:\")\n",
    "visualize_predictions(alexnet_model, test_dataset, device, cifar10_classes, num_images=16, show_correct=True)\n",
    "\n",
    "# Visualize incorrect predictions for AlexNet\n",
    "print(\"\\nAlexNet - Incorrect Predictions:\")\n",
    "visualize_predictions(alexnet_model, test_dataset, device, cifar10_classes, num_images=16, show_correct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.7 Model Performance Comparison Visualization\n",
    "\n",
    "def create_performance_dashboard(models_data):\n",
    "    \"\"\"\n",
    "    Create a comprehensive performance dashboard comparing multiple models.\n",
    "    \n",
    "    Args:\n",
    "        models_data (list): List of tuples (model_name, history, test_accuracy, metrics)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Extract data\n",
    "    model_names = [data[0] for data in models_data]\n",
    "    histories = [data[1] for data in models_data]\n",
    "    test_accs = [data[2] for data in models_data]\n",
    "    metrics_list = [data[3] for data in models_data]\n",
    "    \n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "    \n",
    "    # 1. Final Performance Comparison (Bar chart)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    x_pos = np.arange(len(model_names))\n",
    "    bars = ax1.bar(x_pos, test_accs, color=colors[:len(model_names)], \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Test Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(model_names, fontsize=10)\n",
    "    ax1.set_ylim([0, 100])\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Training Progress (Line plot)\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    epochs = range(1, len(histories[0]['val_acc']) + 1)\n",
    "    for idx, (name, history) in enumerate(zip(model_names, histories)):\n",
    "        ax2.plot(epochs, history['val_acc'], marker='o', linewidth=2, \n",
    "                label=name, color=colors[idx], markersize=4)\n",
    "    ax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Validation Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Validation Accuracy Progress', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Average Precision per Model\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    avg_precisions = [np.mean(m['precision']) for m in metrics_list]\n",
    "    bars3 = ax3.barh(model_names, avg_precisions, color=colors[:len(model_names)],\n",
    "                     edgecolor='black', linewidth=1.5)\n",
    "    ax3.set_xlabel('Average Precision', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('Average Precision Comparison', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlim([0, 1])\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "    for i, bar in enumerate(bars3):\n",
    "        width = bar.get_width()\n",
    "        ax3.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 4. Average Recall per Model\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    avg_recalls = [np.mean(m['recall']) for m in metrics_list]\n",
    "    bars4 = ax4.barh(model_names, avg_recalls, color=colors[:len(model_names)],\n",
    "                     edgecolor='black', linewidth=1.5)\n",
    "    ax4.set_xlabel('Average Recall', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Average Recall Comparison', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlim([0, 1])\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    for i, bar in enumerate(bars4):\n",
    "        width = bar.get_width()\n",
    "        ax4.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 5. Average F1-Score per Model\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    avg_f1s = [np.mean(m['f1']) for m in metrics_list]\n",
    "    bars5 = ax5.barh(model_names, avg_f1s, color=colors[:len(model_names)],\n",
    "                     edgecolor='black', linewidth=1.5)\n",
    "    ax5.set_xlabel('Average F1-Score', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('Average F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "    ax5.set_xlim([0, 1])\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "    for i, bar in enumerate(bars5):\n",
    "        width = bar.get_width()\n",
    "        ax5.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 6. Training vs Validation Loss (final epoch)\n",
    "    ax6 = fig.add_subplot(gs[2, :])\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    train_losses = [h['train_loss'][-1] for h in histories]\n",
    "    val_losses = [h['val_loss'][-1] for h in histories]\n",
    "    \n",
    "    ax6.bar(x - width/2, train_losses, width, label='Training Loss', \n",
    "            color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "    ax6.bar(x + width/2, val_losses, width, label='Validation Loss', \n",
    "            color='coral', edgecolor='black', linewidth=1.5)\n",
    "    ax6.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
    "    ax6.set_title('Final Training vs Validation Loss', fontsize=12, fontweight='bold')\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(model_names)\n",
    "    ax6.legend(fontsize=10)\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Model Performance Dashboard', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\\\nComprehensive Performance Summary:\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Model':<12} {'Test Acc':<12} {'Avg Precision':<15} {'Avg Recall':<12} \"\n",
    "          f\"{'Avg F1':<10} {'Final Train Loss':<17}\")\n",
    "    print(\"-\"*100)\n",
    "    for i, name in enumerate(model_names):\n",
    "        print(f\"{name:<12} {test_accs[i]:>9.2f}% {avg_precisions[i]:>13.4f}  \"\n",
    "              f\"{avg_recalls[i]:>10.4f}  {avg_f1s[i]:>8.4f}  \"\n",
    "              f\"{histories[i]['train_loss'][-1]:>14.4f}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "# Create performance dashboard\n",
    "models_data = [\n",
    "    ('LeNet-5', history_lenet, accuracy_lenet, metrics_lenet),\n",
    "    ('AlexNet', history_alexnet, accuracy_alexnet, metrics_alexnet)\n",
    "]\n",
    "\n",
    "create_performance_dashboard(models_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Saving and Loading Models\n",
    "\n",
    "PyTorch provides flexible ways to save and load models for later use.\n",
    "\n",
    "### Model Persistence Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Trained Model] --> B{What to Save?}\n",
    "    B -->|State Dict| C[torch.save<br/>model.state_dict]\n",
    "    B -->|Entire Model| D[torch.save<br/>model]\n",
    "    B -->|Checkpoint| E[torch.save<br/>full checkpoint]\n",
    "\n",
    "    C --> F[torch.load]\n",
    "    D --> G[torch.load]\n",
    "    E --> H[torch.load]\n",
    "\n",
    "    F --> I[model.load_state_dict]\n",
    "    I --> J[Loaded Model]\n",
    "    G --> J\n",
    "    H --> K[Resume Training]\n",
    "\n",
    "    style C fill:#e1f5ff\n",
    "    style E fill:#ffe1e1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11.1 Saving Models\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('./saved_models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Saving Models ===\\n\")\n",
    "\n",
    "# Method 1: Save only the model parameters (state_dict) - RECOMMENDED\n",
    "# This is the most common and flexible approach\n",
    "# torch.save(obj, path) - saves a serialized object to disk\n",
    "# Parameters:\n",
    "#   obj: object to save (state_dict, model, or any Python object)\n",
    "#   path: file path to save to\n",
    "torch.save(alexnet_model.state_dict(), models_dir / 'alexnet_state_dict.pth')\n",
    "print(\"âœ“ Saved AlexNet state_dict to 'saved_models/alexnet_state_dict.pth'\")\n",
    "\n",
    "# Method 2: Save entire model (less flexible, not recommended for production)\n",
    "torch.save(lenet_model, models_dir / 'lenet_entire_model.pth')\n",
    "print(\"âœ“ Saved LeNet entire model to 'saved_models/lenet_entire_model.pth'\")\n",
    "\n",
    "# Method 3: Save training checkpoint (for resuming training)\n",
    "# This saves everything needed to resume training\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': alexnet_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_alexnet.state_dict(),\n",
    "    'train_loss': history_alexnet['train_loss'][-1],\n",
    "    'val_loss': history_alexnet['val_loss'][-1],\n",
    "    'val_accuracy': history_alexnet['val_acc'][-1],\n",
    "    'history': history_alexnet\n",
    "}\n",
    "torch.save(checkpoint, models_dir / 'alexnet_checkpoint.pth')\n",
    "print(\"âœ“ Saved AlexNet checkpoint to 'saved_models/alexnet_checkpoint.pth'\")\n",
    "\n",
    "print(f\"\\nSaved files:\")\n",
    "for file in sorted(models_dir.iterdir()):\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {file.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11.2 Loading Models\n",
    "\n",
    "print(\"=== Loading Models ===\\n\")\n",
    "\n",
    "# Method 1: Load state_dict (RECOMMENDED)\n",
    "# First, create a new instance of the model\n",
    "loaded_alexnet = AlexNet(num_classes=10).to(device)\n",
    "\n",
    "# torch.load(path, map_location) - loads a saved object\n",
    "# Parameters:\n",
    "#   path: file path to load from\n",
    "#   map_location: device to map tensors to (e.g., 'cpu', 'cuda:0')\n",
    "#   weights_only: if True, only loads weights (safer, prevents code execution)\n",
    "state_dict = torch.load(models_dir / 'alexnet_state_dict.pth', \n",
    "                        map_location=device, weights_only=True)\n",
    "\n",
    "# model.load_state_dict(state_dict, strict=True)\n",
    "# Parameters:\n",
    "#   state_dict: dictionary containing parameters and buffers\n",
    "#   strict: whether to strictly enforce that keys match (default: True)\n",
    "loaded_alexnet.load_state_dict(state_dict)\n",
    "loaded_alexnet.eval()  # Set to evaluation mode\n",
    "print(\"âœ“ Loaded AlexNet from state_dict\")\n",
    "\n",
    "# Verify the loaded model works\n",
    "test_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "with torch.no_grad():\n",
    "    output = loaded_alexnet(test_input)\n",
    "    print(f\"  Test output shape: {output.shape}\")\n",
    "    print(f\"  Predicted class: {output.argmax(1).item()}\")\n",
    "\n",
    "# Method 2: Load entire model\n",
    "loaded_lenet = torch.load(models_dir / 'lenet_entire_model.pth', \n",
    "                          map_location=device, weights_only=False)\n",
    "loaded_lenet.eval()\n",
    "print(\"\\nâœ“ Loaded LeNet entire model\")\n",
    "\n",
    "# Method 3: Load checkpoint (for resuming training)\n",
    "checkpoint = torch.load(models_dir / 'alexnet_checkpoint.pth', \n",
    "                        map_location=device, weights_only=False)\n",
    "\n",
    "# Create model and optimizer\n",
    "resumed_model = AlexNet(num_classes=10).to(device)\n",
    "resumed_optimizer = optim.Adam(resumed_model.parameters())\n",
    "\n",
    "# Load states\n",
    "resumed_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "resumed_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "history = checkpoint['history']\n",
    "\n",
    "print(f\"\\nâœ“ Loaded checkpoint from epoch {start_epoch}\")\n",
    "print(f\"  Last validation accuracy: {checkpoint['val_accuracy']:.2f}%\")\n",
    "print(f\"  Last training loss: {checkpoint['train_loss']:.4f}\")\n",
    "print(f\"  Last validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"\\nCheckpoint can now be used to resume training from epoch {start_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "#### 1. **PyTorch Fundamentals**\n",
    "\n",
    "- **Tensors**: Multi-dimensional arrays with GPU support\n",
    "- **Autograd**: Automatic differentiation for backpropagation\n",
    "- **nn.Module**: Base class for all neural networks\n",
    "- **Optimizers**: Algorithms for updating model parameters\n",
    "\n",
    "#### 2. **Model Architecture**\n",
    "\n",
    "- **LeNet-5**: Simple CNN with 2 conv layers, 3 FC layers (~60K parameters)\n",
    "  - Good for simple image classification tasks\n",
    "  - Fast training, lower accuracy on complex datasets\n",
    "- **AlexNet**: Deeper CNN with 5 conv layers, 3 FC layers (~2-3M parameters)\n",
    "  - Better for complex image classification\n",
    "  - Higher capacity, better accuracy, but slower training\n",
    "\n",
    "#### 3. **Training Pipeline**\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Training mode\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    model.eval()  # Evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        # Validation loop\n",
    "```\n",
    "\n",
    "#### 4. **Best Practices**\n",
    "\n",
    "**Data Preparation:**\n",
    "\n",
    "- Normalize inputs using dataset statistics\n",
    "- Apply data augmentation for training (RandomCrop, RandomHorizontalFlip)\n",
    "- Use separate transforms for training and testing\n",
    "- Split data into train/validation/test sets\n",
    "\n",
    "**Model Training:**\n",
    "\n",
    "- Always call `optimizer.zero_grad()` before backward pass\n",
    "- Use `model.train()` for training, `model.eval()` for evaluation\n",
    "- Use `torch.no_grad()` during inference to save memory\n",
    "- Monitor both training and validation metrics\n",
    "- Use learning rate scheduling for better convergence\n",
    "- Save checkpoints regularly\n",
    "\n",
    "**Debugging:**\n",
    "\n",
    "- Check tensor shapes at each layer\n",
    "- Verify data loading with sample batches\n",
    "- Start with a small learning rate\n",
    "- Use gradient clipping if gradients explode\n",
    "- Monitor loss values (should decrease)\n",
    "\n",
    "**Performance Optimization:**\n",
    "\n",
    "- Use GPU when available (`model.to(device)`)\n",
    "- Set `num_workers > 0` in DataLoader (except on Windows)\n",
    "- Use `pin_memory=True` for faster GPU transfer\n",
    "- Batch multiple samples together\n",
    "- Use mixed precision training for large models\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "1. **Forgetting to zero gradients**: Gradients accumulate by default\n",
    "2. **Not switching to eval mode**: Dropout/BatchNorm behave differently\n",
    "3. **Using wrong device**: Ensure model and data are on same device\n",
    "4. **Incorrect tensor shapes**: Always verify dimensions\n",
    "5. **Learning rate too high**: Can cause training instability\n",
    "6. **No validation set**: Can't detect overfitting\n",
    "7. **Saving entire model vs state_dict**: Use state_dict for flexibility\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue learning PyTorch:\n",
    "\n",
    "1. **Advanced Architectures**: ResNet, VGG, EfficientNet\n",
    "2. **Transfer Learning**: Use pre-trained models on ImageNet\n",
    "3. **Custom Datasets**: Create your own dataset classes\n",
    "4. **Advanced Optimizers**: AdamW, RAdam, Ranger\n",
    "5. **Learning Rate Schedulers**: CosineAnnealing, OneCycleLR\n",
    "6. **Mixed Precision Training**: Faster training with torch.cuda.amp\n",
    "7. **Distributed Training**: Multi-GPU training\n",
    "8. **Other Domains**: NLP (transformers), RL, GANs\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# Essential imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create model\n",
    "model = MyModel().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed this comprehensive PyTorch tutorial. You now have the knowledge to build, train, and evaluate deep learning models for computer vision tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Additional Examples and Tips\n",
    "\n",
    "### A. Custom Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A.1 Custom Loss Function Example\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance.\n",
    "    \n",
    "    Focal Loss = -Î±(1-pt)^Î³ * log(pt)\n",
    "    \n",
    "    where pt is the model's estimated probability for the true class.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Weighting factor (default: 0.25)\n",
    "        gamma (float): Focusing parameter (default: 2.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: (batch_size, num_classes) - raw logits\n",
    "            targets: (batch_size,) - class indices\n",
    "        \"\"\"\n",
    "        # Get probabilities\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)  # Probability of true class\n",
    "        \n",
    "        # Focal loss formula\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Example usage\n",
    "focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "dummy_logits = torch.randn(4, 10)\n",
    "dummy_targets = torch.randint(0, 10, (4,))\n",
    "loss = focal_loss(dummy_logits, dummy_targets)\n",
    "print(f\"Focal Loss: {loss.item():.4f}\")\n",
    "print(\"\\\\nFocal Loss is useful when dealing with imbalanced datasets,\")\n",
    "print(\"as it down-weights easy examples and focuses on hard examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Regularization Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### B.1 Dropout and Batch Normalization\n",
    "\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Example network with Dropout and Batch Normalization.\n",
    "    \n",
    "    Regularization techniques help prevent overfitting:\n",
    "    - Dropout: Randomly drops neurons during training\n",
    "    - Batch Normalization: Normalizes layer inputs\n",
    "    - L2 Regularization: Added through optimizer weight_decay parameter\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, num_classes=10):\n",
    "        super(RegularizedNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        # nn.BatchNorm1d(num_features) - normalizes batch of features\n",
    "        # Parameters:\n",
    "        #   num_features: number of features (same as output of previous layer)\n",
    "        #   momentum: value used for running mean/var (default: 0.1)\n",
    "        #   eps: small value for numerical stability (default: 1e-5)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Normalize\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)  # Dropout (only active during training)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Output layer (no dropout or batch norm)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "reg_model = RegularizedNet()\n",
    "print(\"Regularized Network:\")\n",
    "print(reg_model)\n",
    "print(f\"\\\\nTotal parameters: {sum(p.numel() for p in reg_model.parameters()):,}\")\n",
    "\n",
    "# Test behavior in training vs evaluation mode\n",
    "reg_model.train()\n",
    "dummy_input = torch.randn(8, 784)\n",
    "output_train = reg_model(dummy_input)\n",
    "print(f\"\\\\nTraining mode output (with dropout): shape {output_train.shape}\")\n",
    "\n",
    "reg_model.eval()\n",
    "output_eval = reg_model(dummy_input)\n",
    "print(f\"Evaluation mode output (no dropout): shape {output_eval.shape}\")\n",
    "\n",
    "print(\"\\\\nNote: Dropout is only active in training mode!\")\n",
    "print(\"Always use model.train() for training and model.eval() for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. GPU Utilization and Performance Tips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### C.1 Device Management and GPU Utilization\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=== Device Information ===\\\\n\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # GPU memory management\n",
    "    print(f\"\\\\n=== GPU Memory ===\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "\n",
    "print(\"\\\\n=== Performance Tips ===\\\\n\")\n",
    "\n",
    "# 1. Moving data to GPU efficiently\n",
    "print(\"1. Moving data to GPU:\")\n",
    "print(\"   - Move model to GPU once: model.to(device)\")\n",
    "print(\"   - Move batches in training loop: inputs.to(device)\")\n",
    "print(\"   - Use pin_memory=True in DataLoader for faster transfer\")\n",
    "print()\n",
    "\n",
    "# 2. Batch size optimization\n",
    "print(\"2. Batch size optimization:\")\n",
    "print(\"   - Larger batch = better GPU utilization\")\n",
    "print(\"   - But: may not fit in memory, may harm generalization\")\n",
    "print(\"   - Start with 32-128 and increase until GPU memory is ~80% full\")\n",
    "print()\n",
    "\n",
    "# 3. Clear cache when needed\n",
    "print(\"3. Memory management:\")\n",
    "print(\"   - Clear unused tensors: del tensor\")\n",
    "print(\"   - Clear GPU cache: torch.cuda.empty_cache()\")\n",
    "print(\"   - Use torch.no_grad() for inference\")\n",
    "print()\n",
    "\n",
    "# 4. Mixed precision training\n",
    "print(\"4. Mixed precision training (for faster training):\")\n",
    "print(\"   from torch.cuda.amp import autocast, GradScaler\")\n",
    "print(\"   scaler = GradScaler()\")\n",
    "print(\"   with autocast():\")\n",
    "print(\"       outputs = model(inputs)\")\n",
    "print()\n",
    "\n",
    "# Example: Memory-efficient inference\n",
    "def predict_batch(model, inputs, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Memory-efficient batch prediction.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        inputs: Input tensor\n",
    "        device: Device to use\n",
    "        batch_size: Batch size for inference\n",
    "        \n",
    "    Returns:\n",
    "        Predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch = inputs[i:i+batch_size].to(device)\n",
    "            output = model(batch)\n",
    "            predictions.append(output.cpu())  # Move back to CPU\n",
    "    \n",
    "    return torch.cat(predictions)\n",
    "\n",
    "print(\"5. Example function for memory-efficient inference defined above â†‘\")\n",
    "\n",
    "# Benchmark example\n",
    "if torch.cuda.is_available():\n",
    "    import time\n",
    "    \n",
    "    print(\"\\\\n=== Quick Benchmark ===\\\\n\")\n",
    "    model_bench = AlexNet(num_classes=10)\n",
    "    dummy_data = torch.randn(128, 3, 32, 32)\n",
    "    \n",
    "    # CPU benchmark\n",
    "    model_bench_cpu = model_bench.to('cpu')\n",
    "    dummy_data_cpu = dummy_data.to('cpu')\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model_bench_cpu(dummy_data_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    # GPU benchmark\n",
    "    model_bench_gpu = model_bench.to('cuda')\n",
    "    dummy_data_gpu = dummy_data.to('cuda')\n",
    "    \n",
    "    # Warm up GPU\n",
    "    with torch.no_grad():\n",
    "        _ = model_bench_gpu(dummy_data_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model_bench_gpu(dummy_data_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"CPU inference time: {cpu_time*1000:.2f} ms\")\n",
    "    print(f\"GPU inference time: {gpu_time*1000:.2f} ms\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(\"\\\\nGPU not available for benchmark\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
