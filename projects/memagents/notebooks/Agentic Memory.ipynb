{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memories: Short-Term & Long-Term\n",
        "\n",
        "We will use the `Memory` class (from LlamaIndex)to store and retrieve both short-term and long-term memory.\n",
        "\n",
        "You can use it on its own and orchestrate within a custom workflow, or use it within an existing agent.\n",
        "\n",
        "By default, short-term memory is represented as a FIFO queue of `ChatMessage` objects. Once the queue exceeds a certain size, the last X messages within a flush size are archived and optionally flushed to long-term memory blocks.\n",
        "\n",
        "Long-term memory is represented as `Memory Block` objects. These objects receive the messages that are flushed from short-term memory, and optionally process them to extract information. Then when memory is retrieved, the short-term and long-term memories are merged together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "project root: /Users/pmui/SynologyDrive/research/2026/research2026/projects/memagents\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Resolve project root robustly by finding the folder that contains `asdrp/`\n",
        "PROJECT_ROOT = None\n",
        "for candidate in [Path.cwd(), *Path.cwd().parents]:\n",
        "    if (candidate / \"asdrp\").exists():\n",
        "        PROJECT_ROOT = candidate\n",
        "        break\n",
        "\n",
        "if PROJECT_ROOT is None:\n",
        "    raise RuntimeError(\"Could not find repo root containing 'asdrp'.\")\n",
        "\n",
        "print(f\"project root: {PROJECT_ROOT}\")\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "        \n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "llm = OpenAI(model=\"gpt-5-mini\", temperature=0.01)\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "Settings.embed_model = embed_model\n",
        "Settings.llm = llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Short-term Memory\n",
        "\n",
        "Let's explore how to configure various components of short-term memory.\n",
        "\n",
        "For visual purposes, we will set some low token limits to more easily observe the memory behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core.memory import Memory\n",
        "\n",
        "memory = Memory.from_defaults(\n",
        "    session_id=\"my_session\",\n",
        "    token_limit=50,  # small enough to observe the memory behavior\n",
        "    token_flush_size=10,\n",
        "    chat_history_token_ratio=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's review the configuration we used and what it means:\n",
        "\n",
        "- `session_id`: A unique identifier for the session. Used to mark chat messages in a SQL database as belonging to a specific session.\n",
        "- `token_limit`: The maximum number of tokens that can be stored in short-term + long-term memory.\n",
        "- `chat_history_token_ratio`: The ratio of tokens in the short-term chat history to the total token limit. Here this means that 50\\*0.7 = 35 tokens are allocated to short-term memory, and the rest is allocated to long-term memory.\n",
        "- `token_flush_size`: The number of tokens to flush to long-term memory when the token limit is exceeded. Note that we did not configure long-term memory, so these messages are merely archived in the database and removed from the short-term memory.\n",
        "\n",
        "Using our memory, we can manually add some messages and observe how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "# Simulate a long conversation\n",
        "for i in range(100):\n",
        "    await memory.aput_messages(\n",
        "        [\n",
        "            ChatMessage(role=\"user\", content=\"Hello, world!  Message \" + str(i)),\n",
        "            ChatMessage(role=\"assistant\", content=\"Hello, world to you too!  Message \" + str(i)),\n",
        "            ChatMessage(role=\"user\", content=\"What is the capital of France?  Message \" + str(i)),\n",
        "            ChatMessage(\n",
        "                role=\"assistant\", content=\"The capital of France is Paris.  Message \" + str(i)\n",
        "            ),\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since our token limit is small, we will only see the last 2 messages in short-term memory (since this fits withint the `50*0.7` limit)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_chat_history = await memory.aget()\n",
        "for msg in current_chat_history:\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we retrieva all messages, we will find all 400 messages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "all_messages = await memory.aget_all()\n",
        "print(len(all_messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can clear the memory at any time to start fresh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await memory.areset()\n",
        "all_messages = await memory.aget_all()\n",
        "print(len(all_messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Long-term Memory\n",
        "\n",
        "Long-term memory is represented as Memory Block objects. These objects receive the messages that are flushed from short-term memory, and optionally process them to extract information. Then when memory is retrieved, the short-term and long-term memories are merged together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have 3 prebuilt memory blocks:\n",
        "\n",
        "- `StaticMemoryBlock`: A memory block that stores a static piece of information.\n",
        "- `FactExtractionMemoryBlock`: A memory block that extracts facts from the chat history.\n",
        "- `VectorMemoryBlock`: A memory block that stores and retrieves batches of chat messages from a vector database.\n",
        "\n",
        "Each block has a `priority` that is used when the long-term memory + short-term memory exceeds the token limit. Priority 0 means the block will always be kept in memory, priority 1 means the block will be temporarily disabled, and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.memory import (\n",
        "    StaticMemoryBlock,\n",
        "    FactExtractionMemoryBlock,\n",
        "    VectorMemoryBlock,\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4.1-mini\")\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "client = chromadb.EphemeralClient()\n",
        "vector_store = ChromaVectorStore(\n",
        "    chroma_collection=client.get_or_create_collection(\"test_collection\")\n",
        ")\n",
        "\n",
        "blocks = [\n",
        "    StaticMemoryBlock(\n",
        "        name=\"core_info\",\n",
        "        static_content=\"My name is ASDRP Agent.  I live in Fremont, CA and I love to talk about nested Matryoshka dolls.\",\n",
        "        priority=0,\n",
        "    ),\n",
        "    FactExtractionMemoryBlock(\n",
        "        name=\"extracted_info\",\n",
        "        llm=llm,\n",
        "        max_facts=50,\n",
        "        priority=1,\n",
        "    ),\n",
        "    VectorMemoryBlock(\n",
        "        name=\"vector_memory\",\n",
        "        # required: pass in a vector store like qdrant, chroma, weaviate, milvus, etc.\n",
        "        vector_store=vector_store,\n",
        "        priority=2,\n",
        "        embed_model=embed_model,\n",
        "        # The top-k message batches to retrieve\n",
        "        # similarity_top_k=2,\n",
        "        # optional: How many previous messages to include in the retrieval query\n",
        "        # retrieval_context_window=5\n",
        "        # optional: pass optional node-postprocessors for things like similarity threshold, etc.\n",
        "        # node_postprocessors=[...],\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our blocks created, we can pass them into the Memory class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.memory import Memory\n",
        "\n",
        "memory = Memory.from_defaults(\n",
        "    session_id=\"my_session\",\n",
        "    token_limit=30000,\n",
        "    # Setting a extremely low ratio so that more tokens are flushed to long-term memory\n",
        "    chat_history_token_ratio=0.02,\n",
        "    token_flush_size=500,\n",
        "    memory_blocks=blocks,\n",
        "    # insert into the latest user message, can also be \"system\"\n",
        "    insert_method=\"user\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this, we can simulate a conversation with an agent and inspect the long-term memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "agent = FunctionAgent(\n",
        "    tools=[],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "user_msgs = [\n",
        "    \"Hi! My name is Jerry\",\n",
        "    \"What is your opinion on nested Matryoshka dolls?\",\n",
        "    \"What is the most popular nesting doll?\",\n",
        "    \"In history, what is the most significant nesting doll?\",\n",
        "    \"What is the most expensive nesting doll?\",\n",
        "    \"I am interested in buying a nesting doll, what is the most popular nesting doll?\",\n",
        "    \"What is the most valuable nesting doll?\",\n",
        "    \"Last week, I bought a nesting doll.\",\n",
        "    \"What is the most rare nesting doll?\",\n",
        "    \"I am thinking about the historical significance of nesting dolls, what is the most interesting nesting doll?\",\n",
        "    \"What is the most unique nesting doll?\",\n",
        "    \"Why are nesting dolls so popular?\",\n",
        "    \"What is the most interesting nesting doll?\",\n",
        "]\n",
        "\n",
        "for user_msg in user_msgs:\n",
        "    _ = await agent.run(user_msg=user_msg, memory=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's inspect the most recent user-message and see what the memory inserts into the user message.\n",
        "\n",
        "Note that we pass in at least one chat message so that the vector memory actually runs retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history = await memory.aget()\n",
        "for chat in chat_history:\n",
        "    print(f\"==> {chat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, we can see that the current FIFO queue is only 2-3 messages (expected since we set the chat history token ratio to 0.02).\n",
        "\n",
        "Now, let's inspect the long-term memory blocks that are inserted into the latest user message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for block in chat_history[-2].blocks:\n",
        "    print(block.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use this memory outside an agent, and to highlight more of the usage, you might do something like the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_user_msg = ChatMessage(\n",
        "    role=\"user\", content=\"What kind of doll was I asking about?\"\n",
        ")\n",
        "await memory.aput(new_user_msg)\n",
        "\n",
        "# Get the new chat history\n",
        "new_chat_history = await memory.aget()\n",
        "resp = await llm.achat(new_chat_history)\n",
        "await memory.aput(resp.message)\n",
        "print(resp.message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Mem0Agent: LlamaIndex + Mem0 Memory\n",
        "\n",
        "This section demonstrates a full Mem0-enabled agent using the LlamaIndex **Mem0Memory** wrapper. It covers setup, memory scoping, direct memory access, and test scenarios.\n",
        "\n",
        "References:\n",
        "\n",
        "- https://docs.mem0.ai/integrations/llama-index\n",
        "- https://docs.mem0.ai/cookbooks/frameworks/llamaindex-react\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "  U[User] -->|message| A[Mem0Agent]\n",
        "  A -->|search context| M[(Mem0 Memory)]\n",
        "  M -->|memories| A\n",
        "  A -->|tool calls| T[Tools]\n",
        "  A -->|response| U\n",
        "  A -->|store new memory| M\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n",
        "\n",
        "Install the Mem0 LlamaIndex integration (and dotenv if needed):\n",
        "\n",
        "```\n",
        "pip install llama-index-core llama-index-memory-mem0 python-dotenv\n",
        "```\n",
        "\n",
        "You will need:\n",
        "\n",
        "- `OPENAI_API_KEY`\n",
        "- `MEM0_API_KEY` (Mem0 Platform) or a Mem0 OSS config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "missing = [k for k in [\"OPENAI_API_KEY\", \"MEM0_API_KEY\"] if not os.getenv(k)]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required environment variables: {missing}\")\n",
        "\n",
        "print(\"Environment ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Scoping with Context\n",
        "\n",
        "Mem0 memory is scoped by `context` so you can isolate or share memory across users, agents, and runs.\n",
        "\n",
        "```mermaid\n",
        "flowchart TB\n",
        "  C[Context] --> U[user_id]\n",
        "  C --> A[agent_id]\n",
        "  C --> R[run_id]\n",
        "  U --> M[(Mem0 Memory)]\n",
        "  A --> M\n",
        "  R --> M\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Suppress Dependency Warnings\n",
        "\n",
        "The following deprecation warnings come from dependencies (LlamaIndex, Pydantic), not your code. We suppress them for cleaner output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress Pydantic deprecation warnings from dependencies\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pydantic\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*PydanticDeprecated.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*utcfromtimestamp.*\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/pmui/.local/share/uv/python/cpython-3.13.3-macos-aarch64-none/lib/python3.13/inspect.py:602: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use the `model_fields` class property instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/pmui/.local/share/uv/python/cpython-3.13.3-macos-aarch64-none/lib/python3.13/inspect.py:602: PydanticDeprecatedSince20: The `__fields_set__` attribute is deprecated, use `model_fields_set` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  value = getter(object, key)\n",
            "/Users/pmui/.local/share/uv/python/cpython-3.13.3-macos-aarch64-none/lib/python3.13/inspect.py:602: PydanticDeprecatedSince211: Accessing the 'model_computed_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n",
            "/Users/pmui/.local/share/uv/python/cpython-3.13.3-macos-aarch64-none/lib/python3.13/inspect.py:602: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
            "  value = getter(object, key)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<asdrp.agent.mem0_agent.Mem0Agent at 0x11446b4d0>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from asdrp.agent.mem0_agent import Mem0Agent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "def call_fn(name: str) -> str:\n",
        "    return f\"Calling {name}\"\n",
        "\n",
        "\n",
        "def email_fn(name: str) -> str:\n",
        "    return f\"Emailing {name}\"\n",
        "\n",
        "\n",
        "call_tool = FunctionTool.from_defaults(fn=call_fn)\n",
        "email_tool = FunctionTool.from_defaults(fn=email_fn)\n",
        "\n",
        "tools = [call_tool, email_tool]\n",
        "\n",
        "agent = Mem0Agent(\n",
        "    context={\"user_id\": \"david\", \"agent_id\": \"mem0_agent\"},\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "\n",
        "async def run_session():\n",
        "    reply = await agent.achat(\"Hi, my name is David.\")\n",
        "    print(reply.response_str)\n",
        "\n",
        "    reply = await agent.achat(\"I prefer email updates.\")\n",
        "    print(reply.response_str)\n",
        "\n",
        "    reply = await agent.achat(\"Please contact me about my next order.\")\n",
        "    print(reply.response_str)\n",
        "\n",
        "await run_session()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Direct Memory Access (Search and Add)\n",
        "\n",
        "`Mem0Agent` exposes simple helpers to search and add memories directly. This is useful for debugging, seeding, or inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # Mem0 API requires non-empty filters for search\n",
        "    results = agent.search_memories(\n",
        "        \"preferred communication\",\n",
        "        limit=3,\n",
        "        filters={\"user_id\": \"david\"},\n",
        "    )\n",
        "    print(results)\n",
        "\n",
        "    agent.add_memories(\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"My favorite cuisine is Italian.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Got it. I will remember that.\"},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    results = agent.search_memories(\n",
        "        \"favorite cuisine\",\n",
        "        limit=3,\n",
        "        filters={\"user_id\": \"david\"},\n",
        "    )\n",
        "    print(results)\n",
        "except Exception as exc:\n",
        "    print(f\"Mem0 backend not available: {exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Isolation vs Shared Context\n",
        "\n",
        "Use different `user_id` values to isolate memory, or reuse the same `user_id` to share memory across agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_a = Mem0Agent(context={\"user_id\": \"shared_user\", \"agent_id\": \"agent_a\"})\n",
        "agent_b = Mem0Agent(context={\"user_id\": \"shared_user\", \"agent_id\": \"agent_b\"})\n",
        "agent_c = Mem0Agent(context={\"user_id\": \"isolated_user\", \"agent_id\": \"agent_c\"})\n",
        "\n",
        "async def share_and_isolate():\n",
        "    await agent_a.achat(\"I like black coffee.\")\n",
        "\n",
        "    reply_shared = await agent_b.achat(\"What coffee do I like?\")\n",
        "    print(\"Shared context:\", reply_shared.response_str)\n",
        "\n",
        "    reply_isolated = await agent_c.achat(\"What coffee do I like?\")\n",
        "    print(\"Isolated context:\", reply_isolated.response_str)\n",
        "\n",
        "await share_and_isolate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Mem0 OSS (Optional)\n",
        "\n",
        "If you run Mem0 OSS locally, configure the vector store, embedder, and LLM in a Mem0 config and initialize `Mem0Agent` with `use_platform=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mem0_config = {\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"qdrant\",\n",
        "        \"config\": {\n",
        "            \"collection_name\": \"mem0_demo\",\n",
        "            \"host\": \"localhost\",\n",
        "            \"port\": 6333,\n",
        "            \"embedding_model_dims\": 1536,\n",
        "        },\n",
        "    },\n",
        "    \"llm\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gpt-4.1-nano-2025-04-14\",\n",
        "            \"temperature\": 0.2,\n",
        "            \"max_tokens\": 2000,\n",
        "        },\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"config\": {\"model\": \"text-embedding-3-small\"},\n",
        "    },\n",
        "    \"version\": \"v1.1\",\n",
        "}\n",
        "\n",
        "# oss_agent = Mem0Agent(\n",
        "#     context={\"user_id\": \"oss_user\", \"agent_id\": \"mem0_agent\"},\n",
        "#     mem0_config=mem0_config,\n",
        "#     use_platform=False,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Checklist\n",
        "\n",
        "- Verify the agent remembers preferences across multiple sessions.\n",
        "- Confirm `search_memories()` returns expected memories.\n",
        "- Compare shared vs isolated contexts (`user_id`).\n",
        "- Validate tool use with memory-backed prompts.\n",
        "- Measure latency and token usage with and without Mem0.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
